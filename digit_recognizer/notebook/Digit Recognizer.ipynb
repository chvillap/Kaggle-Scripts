{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer\n",
    "This is just the notebook that I wrote in order to run my Convolutional Neural Network model in an Apache Spark virtual cluster (powered by [Databricks](https://databricks.com/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install nolearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nolearn.lasagne import BatchIterator\n",
    "from scipy.ndimage.interpolation import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RotateBatchIterator(BatchIterator):\n",
    "    \"\"\"Performs data augmentation by rotating half of the images in the batch.\n",
    "    \"\"\"\n",
    "    def transform(self, Xb, yb):\n",
    "        Xb, yb = super(RotateBatchIterator, self).transform(Xb, yb)\n",
    "        batch_size = Xb.shape[0]\n",
    "        indices = np.random.choice(batch_size, batch_size / 2, replace=False)\n",
    "        ind_pos = indices[0::2]\n",
    "        ind_neg = indices[1::2]\n",
    "        Xb[ind_pos] = rotate(Xb[ind_pos], angle=30.0, axes=(3, 2),\n",
    "            reshape=False, order=1)\n",
    "        Xb[ind_neg] = rotate(Xb[ind_neg], angle=-30.0, axes=(3, 2),\n",
    "            reshape=False, order=1)\n",
    "        return Xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdjustParameter(object):\n",
    "    \"\"\"Adjusts the value of some parameter of the network at the end of each\n",
    "    epoch, in order to make it vary over a range of predefined values.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, start, stop):\n",
    "        self.name = name\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = np.cast['float32'](self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    \"\"\"Stops the learning process early if the network spends too many epochs\n",
    "    without any performance improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.best_valid = np.Inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        current_valid = train_history[-1]['valid_loss']\n",
    "        current_epoch = train_history[-1]['epoch']\n",
    "\n",
    "        if current_valid < self.best_valid:\n",
    "            self.best_valid = current_valid\n",
    "            self.best_valid_epoch = current_epoch\n",
    "            self.best_weights = nn.get_all_params_values()\n",
    "\n",
    "        elif self.best_valid_epoch + self.patience < current_epoch:\n",
    "            print('Early stopping')\n",
    "            print('Best valid loss was %f at epoch %d' %\n",
    "                  (self.best_valid, self.best_valid_epoch))\n",
    "            nn.load_params_from(self.best_weights)\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    \"\"\"Trains a ConvNet classifier on some training data.\n",
    "\n",
    "    Inputs:\n",
    "        X  Training data.\n",
    "        y  Training labels.\n",
    "\n",
    "    Outputs:\n",
    "        clf  Trained classifier.\n",
    "    \"\"\"\n",
    "    from theano import shared\n",
    "    from nolearn.lasagne import NeuralNet\n",
    "    from nolearn.lasagne import TrainSplit\n",
    "    from lasagne.layers import InputLayer\n",
    "    from lasagne.layers import Conv2DLayer\n",
    "    from lasagne.layers import DenseLayer\n",
    "    from lasagne.layers import MaxPool2DLayer\n",
    "    from lasagne.layers import DropoutLayer\n",
    "    from lasagne.nonlinearities import softmax\n",
    "\n",
    "    layers = [\n",
    "        (InputLayer, {'shape': (None, X.shape[1], X.shape[2], X.shape[3])}),\n",
    "        (Conv2DLayer, {'num_filters': 32, 'filter_size': 3, 'pad': 'same'}),\n",
    "        (MaxPool2DLayer, {'pool_size': 2}),\n",
    "        (DropoutLayer, {'p': 0.1}),\n",
    "        (Conv2DLayer, {'num_filters': 64, 'filter_size': 3, 'pad': 'same'}),\n",
    "        (MaxPool2DLayer, {'pool_size': 2}),\n",
    "        (DropoutLayer, {'p': 0.2}),\n",
    "        (Conv2DLayer, {'num_filters': 128, 'filter_size': 3, 'pad': 'same'}),\n",
    "        (MaxPool2DLayer, {'pool_size': 2}),\n",
    "        (DropoutLayer, {'p': 0.3}),\n",
    "        (DenseLayer, {'num_units': 512}),\n",
    "        (DropoutLayer, {'p': 0.4}),\n",
    "        (DenseLayer, {'num_units': 256}),\n",
    "        (DropoutLayer, {'p': 0.5}),\n",
    "        (DenseLayer, {'num_units': 10, 'nonlinearity': softmax}),\n",
    "    ]\n",
    "    clf = NeuralNet(\n",
    "        layers,\n",
    "        max_epochs=100,\n",
    "        train_split=TrainSplit(eval_size=0.25),\n",
    "        # batch_iterator_train=RotateBatchIterator(batch_size=128),\n",
    "        on_epoch_finished=[\n",
    "            AdjustParameter('update_learning_rate', start=0.01, stop=0.0001),\n",
    "            AdjustParameter('update_momentum', start=0.9, stop=0.999),\n",
    "            EarlyStopping(patience=10),\n",
    "        ],\n",
    "        update_learning_rate=shared(np.cast['float32'](0.01)),\n",
    "        update_momentum=shared(np.cast['float32'](0.9)),\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    return clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(clf, X, y):\n",
    "    \"\"\"Evaluates a pretrained classifier on some test data.\n",
    "\n",
    "    Inputs:\n",
    "        clf  Pretrained classifier.\n",
    "        X    Test data.\n",
    "        y    Test labels.\n",
    "\n",
    "    Outputs:\n",
    "        y_pred  Array of predicted labels.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    print('ACCURACY:')\n",
    "    print(((y == y_pred).sum() / y.size))\n",
    "    print()\n",
    "\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print()\n",
    "\n",
    "    print('CLASSIFICATION REPORT:')\n",
    "    print(classification_report(y, y_pred))\n",
    "    print()\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # databricks\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nolearn.lasagne.visualize import plot_loss\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN_PATH = 'datasets/train.csv' # localhost\n",
    "# TEST_PATH = 'datasets/test.csv' # localhost\n",
    "TRAIN_PATH = '/FileStore/tables/8iacswxe1475982269318/train.csv' # databricks\n",
    "TEST_PATH = '/FileStore/tables/nrhys71b1475981053444/test.csv' # databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = SparkSession.builder \\\n",
    "                 .master(\"local\") \\\n",
    "                 .appName(\"kaggle_digit_recognizer\") \\\n",
    "                 .config(\"spark.some.config.option\", \"session\") \\\n",
    "                 .getOrCreate() # databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(TRAIN_PATH)\n",
    "rdd = rdd.filter(lambda row: 'pixel0' not in row)\n",
    "rdd = rdd.map(lambda row: [int(x) for x in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(TRAIN_PATH) # localhost\n",
    "colnames = ['label'] + ['pixel%d' % i for i in range(784)] # databricks\n",
    "df_train = ss.createDataFrame(rdd, colnames).toPandas() # databricks\n",
    "del rdd # databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop('label', axis=1).values.astype(np.float32) / 255\n",
    "y_train = df_train['label'].values.astype(np.int32)\n",
    "del df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imsize = (int(sqrt(X_train.shape[1])),) * 2\n",
    "X_train = X_train.reshape(-1, 1, imsize[0], imsize[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = train(X_train, y_train)\n",
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plot_loss(clf)\n",
    "# plt.show() # localhost\n",
    "display(fig) # databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate(clf, X_valid, y_valid)\n",
    "# del X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(TEST_PATH)\n",
    "rdd = rdd.filter(lambda row: 'pixel0' not in row)\n",
    "rdd = rdd.map(lambda row: [int(x) for x in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(TEST_PATH) # localhost\n",
    "colnames = ['pixel%d' % i for i in range(784)] # databricks\n",
    "df_test = ss.createDataFrame(rdd, colnames).toPandas() # databricks\n",
    "del rdd # databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = df_test.values.astype(np.float32) / 255\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1, 1, imsize[0], imsize[1])\n",
    "y_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imid = np.arange(1, y_test.size + 1)\n",
    "df_submission = pd.DataFrame({'ImageId': imid, 'Label': y_test})\n",
    "# df_submission.to_csv('submission.csv') # localhost\n",
    "display(ss.createDataFrame(df_submission)) # databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "kaggle-digit-recognizer",
  "notebookId": 3363999384393742
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
